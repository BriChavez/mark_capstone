{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpra = 'body of text ex: medical journal ect'\n",
    "lexicon = \"words and their meanings type of speach \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "file_path1 = \"data/\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our\n",
      "Information\n",
      "Security\n",
      "team\n",
      "is\n",
      "growing\n",
      "and\n",
      "hiring\n",
      "an\n",
      "Information\n",
      "Security\n",
      "Analyst\n",
      "II\n",
      "!\n",
      "If\n",
      "you\n",
      "enjoy\n",
      "innovating\n",
      "and\n",
      "developing\n",
      "solutions\n",
      "for\n",
      "information\n",
      "security\n",
      "advancement\n",
      "and\n",
      "searching\n",
      "for\n",
      "potential\n",
      "security\n",
      "threats\n",
      ",\n",
      "this\n",
      "is\n",
      "the\n",
      "job\n",
      "for\n",
      "you\n",
      ".\n",
      "Data\n",
      "security\n",
      "is\n",
      "a\n",
      "top\n",
      "priority\n",
      ",\n",
      "especially\n",
      "in\n",
      "financial\n",
      "institutions\n",
      ".\n",
      "In\n",
      "today\n",
      "’\n",
      "s\n",
      "connected\n",
      "world\n",
      ",\n",
      "access\n",
      "to\n",
      "data\n",
      "is\n",
      "essential\n",
      ",\n",
      "so\n",
      "is\n",
      "protecting\n",
      "it\n",
      ".\n",
      "We\n",
      "have\n",
      "an\n",
      "obligation\n",
      "to\n",
      "keep\n",
      "our\n",
      "member\n",
      "’\n",
      "s\n",
      "and\n",
      "employee\n",
      "’\n",
      "s\n",
      "information\n",
      "safe\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"Our Information Security team is growing and hiring an Information Security Analyst II! If you enjoy innovating and developing solutions for information security advancement and searching for potential security threats, this is the job for you. Data security is a top priority, especially in financial institutions. In today’s connected world, access to data is essential, so is protecting it. We have an obligation to keep our member’s and employee’s information safe.\"\n",
    "\n",
    "# print(sent_tokenize(sample_text))\n",
    "# print(word_tokenize(sample_text))\n",
    "\n",
    "\n",
    "for i in word_tokenize(sample_text):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Our', 'Information', 'Security', 'team', 'growing', 'hiring', 'Information', 'Security', 'Analyst', 'II', '!', 'If', 'enjoy', 'innovating', 'developing', 'solutions', 'information', 'security', 'advancement', 'searching', 'potential', 'security', 'threats', ',', 'job', '.', 'Data', 'security', 'top', 'priority', ',', 'especially', 'financial', 'institutions', '.', 'In', 'today', '’', 'connected', 'world', ',', 'access', 'data', 'essential', ',', 'protecting', '.', 'We', 'obligation', 'keep', 'member', '’', 'employee', '’', 'information', 'safe', '.']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"stopwords\"\"\"\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "# stop word filtration\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "words = word_tokenize(sample_text)\n",
    "\n",
    "# filtered_sentence = []\n",
    "# for w in words:\n",
    "#     if w not in stop_words:\n",
    "#         filtered_sentence.append(w)\n",
    "\"\"\"or do the lambda\"\"\"\n",
    "\n",
    "filtered_sentence = [w for w in words if not w in stop_words]\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['our', 'inform', 'secur', 'team', 'grow', 'hire', 'inform', 'secur', 'analyst', 'ii', '!', 'if', 'enjoy', 'innov', 'develop', 'solut', 'inform', 'secur', 'advanc', 'search', 'potenti', 'secur', 'threat', ',', 'job', '.', 'data', 'secur', 'top', 'prioriti', ',', 'especi', 'financi', 'institut', '.', 'in', 'today', '’', 'connect', 'world', ',', 'access', 'data', 'essenti', ',', 'protect', '.', 'we', 'oblig', 'keep', 'member', '’', 'employe', '’', 'inform', 'safe', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# \"\"\"stemming\"\"\"\n",
    "# takes the root base of words. write instead of writing\n",
    "ps = PorterStemmer()\n",
    "\n",
    "stemmed_sentence = [ps.stem(w) for w in filtered_sentence]\n",
    "\n",
    "# for w in filtered_sentence:\n",
    "#     print(ps.stem(w))\n",
    "\n",
    "print(stemmed_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART OF SPEECH TAGGING\n",
    "import nltk\n",
    "from nltk.corpus import state_union\n",
    "# unsupervized machine learning sentence tokenizer. Comes pretrained, but you can retrain it\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "\n",
    "train_text = state_union.raw('2005-GWBush.txt')\n",
    "sample_text = state_union.raw('2006-GWBush.txt')\n",
    "\n",
    "\n",
    "# make our own tokenizer. Starting its training\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
    "\n",
    "# set our trained tokenizer on the sample text\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
    "\n",
    "\n",
    "# # lets do functions\n",
    "# def process_content():\n",
    "#     try:\n",
    "#         for i in tokenized:\n",
    "#             words = nltk.word_tokenize(i)\n",
    "#             tagged = nltk.pos_tag(words)\n",
    "\n",
    "#             print(tagged)\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(str(e))\n",
    "# process_content()\n",
    "\n",
    "\n",
    "\n",
    "# created tuples of word and part of speach\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6149/1810600813.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mprocess_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_6149/1810600813.py\u001b[0m in \u001b[0;36mprocess_content\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mchunked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchunkParser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtagged\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mchunked\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;31m# print(chunked)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deb/tests/capstone/venv/lib/python3.7/site-packages/nltk/tree/tree.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    760\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdraw_trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m         \u001b[0mdraw_trees\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpretty_print\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhighlight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deb/tests/capstone/venv/lib/python3.7/site-packages/nltk/draw/tree.py\u001b[0m in \u001b[0;36mdraw_trees\u001b[0;34m(*trees)\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1007\u001b[0m     \"\"\"\n\u001b[0;32m-> 1008\u001b[0;31m     \u001b[0mTreeView\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtrees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmainloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1009\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deb/tests/capstone/venv/lib/python3.7/site-packages/nltk/draw/tree.py\u001b[0m in \u001b[0;36mmainloop\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    996\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0min_idle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    997\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 998\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_top\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmainloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    999\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1000\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/tkinter/__init__.py\u001b[0m in \u001b[0;36mmainloop\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   1284\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmainloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m         \u001b[0;34m\"\"\"Call the mainloop of Tk.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1286\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmainloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1287\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mquit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m         \u001b[0;34m\"\"\"Quit the Tcl interpreter. All widgets will be destroyed.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# CHUNKING\n",
    "# uses modifiers mainly\n",
    "import matplotlib\n",
    "import sys\n",
    "# if sys.version_info[0] == 3:\n",
    "import tkinter as tk\n",
    "# else:\n",
    "#     import Tkinter as tk\n",
    "# from matplotlib import draw\n",
    "# lets do functions\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized[5:]:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            # chunks include things and chinks pick things to disinclude\n",
    "            chunkGram = r\"\"\"Chunk: {<.*>+} \n",
    "                            }<VB.?|IN|DT|TO>+{\"\"\"\n",
    "\n",
    "            # parse via chunk gram\n",
    "            chunkParser = nltk.RegexpParser(chunkGram)\n",
    "            chunked = chunkParser.parse(tagged)\n",
    "\n",
    "            chunked.draw()\n",
    "\n",
    "            # print(chunked)\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "process_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NAME RECOGNITION\n",
    "import tkinter as tk\n",
    "\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized[5:]:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "\n",
    "            # calls nltk's name entity chunker on our tagged words\n",
    "            # bianary = true makes them stay as together, usually\n",
    "            namedEnt = nltk.ne_chunk(tagged, binary = True)\n",
    "\n",
    "            namedEnt.draw()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "process_content()\n",
    "\n",
    "# process_content()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# SET RESUME THEME YOU WOULD LIKE TO ACCESS\n",
    "variable = 'test'\n",
    "\n",
    "# compile all the PDF filenames in that folder:\n",
    "folder_path = f'data/Resume/{variable}'\n",
    "# extract folder name based off the variable\n",
    "# ---(i know this is redundant, but I wrote it before the variable and i thought it to be clever, so its still in here)---\n",
    "resume_folder = os.path.basename(folder_path).lower()\n",
    "\n",
    "\n",
    "text_file = open(f'{folder_path}/{resume_folder}_resumes.txt')\n",
    "text_string = text_file.read()\n",
    "\n",
    "\n",
    "bri_file = open('data/bri/bri_resumes.txt')\n",
    "bri_string = bri_file.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "documents = [(list(movie_reviews.words(fileid``````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"LEMETIZING\"\"\"\n",
    "# similar to stemming, but easier to read. uses synonymns\n",
    "# defaults to nouns, unless you state otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize(bri_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"WORDNET\"\"\"\n",
    "# synonymns, definitions, context, anyonymns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "try\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "syns = wordnet.synsets('try')\n",
    "print(syns[6].lemmas()[0].name())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give pain or trouble to\n"
     ]
    }
   ],
   "source": [
    "print(syns[6].definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"I've been sorely tried by these students\"]\n"
     ]
    }
   ],
   "source": [
    "print(syns[6].examples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'slim', 'flimsy', 'thin', 'slender', 'rebuff', 'slight', 'little', 'svelte', 'fragile', 'tenuous', 'cold-shoulder'}\n",
      "{'much'}\n"
     ]
    }
   ],
   "source": [
    "synonyms = []\n",
    "antonyms = []\n",
    "\n",
    "for syn in wordnet.synsets('slight'):\n",
    "    for l in syn.lemmas():\n",
    "        synonyms.append(l.name())\n",
    "        if l.antonyms():\n",
    "            antonyms.append(l.antonyms()[0].name())\n",
    "\n",
    "print(set(synonyms))\n",
    "print(set(antonyms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9090909090909091\n"
     ]
    }
   ],
   "source": [
    "# semantics\n",
    "\n",
    "w1 = wordnet.synset(\"ship.n.01\")\n",
    "w2 = wordnet.synset(\"boat.n.01\")\n",
    "\"\"\"wup stands for woo and palmer, wrote a paper on semantic symalarities and uses their methoids\"\"\"\n",
    "#  returns a value between 0 and 1, one being 100%\n",
    "print(w1.wup_similarity(w2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <> for POS tag\n",
    "\n",
    "# {1,3} = for digits, u expect 1-3 counts of digits, or \"places\"\n",
    "# + = match 1 or more\n",
    "# ? = match 0 or 1 repetitions.\n",
    "# * = match 0 or MORE repetitions\n",
    "# $ = matches at the end of string\n",
    "# ^ = matches start of a string\n",
    "# | = matches either/or. Example x|y = will match either x or y\n",
    "# [] = range, or \"variance\"\n",
    "# {x} = expect to see this amount of the preceding code.\n",
    "# {x,y} = expect to see this x-y amounts of the precedng code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"NE Type and Examples\"\"\"\n",
    "# ORGANIZATION - Georgia-Pacific Corp., WHO\n",
    "# PERSON - Eddy Bonte, President Obama\n",
    "# LOCATION - Murray River, Mount Everest\n",
    "# DATE - June, 2008-06-29\n",
    "# TIME - two fifty a m, 1: 30 p.m.\n",
    "# MONEY - 175 million Canadian Dollars, GBP 10.40\n",
    "# PERCENT - twenty pct, 18.75 %\n",
    "# FACILITY - Washington Monument, Stonehenge\n",
    "# GPE - South East Asia, Midlothian\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"POS tag list:\"\"\"\n",
    "\n",
    "    # CC\tcoordinating conjunction\n",
    "    # CD\tcardinal digit\n",
    "    # DT\tdeterminer\n",
    "    # EX\texistential there(like: \"there is\" ... think of it like \"there exists\")\n",
    "    # FW\tforeign word\n",
    "    # IN\tpreposition/subordinating conjunction\n",
    "    # JJ\tadjective\t'big'\n",
    "    # JJR\tadjective, comparative\t'bigger'\n",
    "    # JJS\tadjective, superlative\t'biggest'\n",
    "    # LS\tlist marker\t1)\n",
    "    #     MD\tmodal\tcould, will\n",
    "    #     NN\tnoun, singular 'desk'\n",
    "    #     NNS\tnoun plural\t'desks'\n",
    "    #     NNP\tproper noun, singular\t'Harrison'\n",
    "    #     NNPS\tproper noun, plural\t'Americans'\n",
    "    #     PDT\tpredeterminer\t'all the kids'\n",
    "    #     POS\tpossessive ending\tparent\\'s\n",
    "    #     PRP\tpersonal pronoun\tI, he, she\n",
    "    #     PRP$\tpossessive pronoun\tmy, his, hers\n",
    "    #     RB\tadverb\tvery, silently,\n",
    "    #     RBR\tadverb, comparative\tbetter\n",
    "    #     RBS\tadverb, superlative\tbest\n",
    "    #     RP\tparticle\tgive up\n",
    "    #     TO\tto\tgo 'to' the store.\n",
    "    #     UH\tinterjection\terrrrrrrrm\n",
    "    #     VB\tverb, base form\ttake\n",
    "    #     VBD\tverb, past tense\ttook\n",
    "    #     VBG\tverb, gerund/present participle\ttaking\n",
    "    #     VBN\tverb, past participle\ttaken\n",
    "    #     VBP\tverb, sing. present, non-3d\ttake\n",
    "    #     VBZ\tverb, 3rd person sing. present\ttakes\n",
    "    #     WDT\twh-determiner\twhich\n",
    "    #     WP\twh-pronoun\twho, what\n",
    "    #     WP$\tpossessive wh-pronoun\twhose\n",
    "    #     WRB\twh-abverb\twhere, when\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4ef022e24af0d9896d2df829234e3a8e308e976d2609f0b1a6e645a4d7a45074"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
