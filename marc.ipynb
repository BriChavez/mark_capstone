{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "import re\n",
    "import string\n",
    "from numpy.random import choice\n",
    "import random\n",
    "import PyPDF2\n",
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "# nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# nltk.download('wordnet')\n",
    "\"\"\"wordnet_tagged = list(map(lambda x: (x[0], pos_tagger(x[1])), pos_tagged))\n",
    "\"\"\"\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "\"\"\"create a dict of how what words follow which and how many times they did\"\"\"\n",
    "with open('data/Resume/test/test_resumes.txt', 'r') as resume_script:\n",
    "    # open txt file and read to string, string to lower\n",
    "    resume_file = resume_script.read()\n",
    "    resume_file = resume_file.lower()\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_words = word_tokenize(resume_file)\n",
    "lemmings = ' '.join([lemmatizer.lemmatize(w) for w in resume_words])\n",
    "# for words in resume_words:\n",
    "#     print(words + ' ---> ' + lemmatizer.lemmatize(words))\n",
    "# print(lemmings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleared our test data of stop words\n",
    "# words = word_tokenize(resume_file)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "token_lemmings = word_tokenize(lemmings)\n",
    "\n",
    "stopped_lemmings = [w for w in token_lemmings if not w in stop_words]\n",
    "# print(stopped_lemmings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = set(nltk.corpus.words.words())\n",
    "\n",
    "drop_odd = [\" \".join(w for w in nltk.wordpunct_tokenize(str(stopped_lemmings)) if w in words)]\n",
    "# print(drop_odd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to lemmatize each word with its POS tag\n",
    "\n",
    "# POS_TAGGER_FUNCTION : TYPE 1\n",
    "def pos_tagger(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pos_tagged = nltk.pos_tag(nltk.word_tokenize(str(drop_odd)))\n",
    "print(pos_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use our own pos_tagger function to make things simpler to understand.\n",
    "wordnet_tagged = list(map(lambda x: (x[0], pos_tagger(x[1])), pos_tagged))\n",
    "\n",
    "\n",
    "wn_tagged_sent = []\n",
    "for word, tag in wordnet_tagged:\n",
    "    if tag is None:\n",
    "        # if there is no available tag, append the token as is\n",
    "        wn_tagged_sent.append(word)\n",
    "    else:\n",
    "        # else use the tag to lemmatize the token\n",
    "        wn_tagged_sent.append(lemmatizer.lemmatize(word, tag))\n",
    "wn_tagged_sent = \" \".join(wn_tagged_sent)\n",
    "\n",
    "print(wn_tagged_sent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "# Define function to lemmatize each word with its POS tag\n",
    "\n",
    "# POS_TAGGER_FUNCTION : TYPE 2\n",
    "drop_odd = str(drop_odd)\n",
    "\n",
    "def pos_tagger(drop_odd):\n",
    "    sent = TextBlob(drop_odd)\n",
    "    tag_dict = {\"J\": 'a', \"N\": 'n', \"V\": 'v', \"R\": 'r'}\n",
    "    words_tags = [(w, tag_dict.get(pos[0], 'n')) for w, pos in sent.tags]\n",
    "    lemma_list = [wd.lemmatizer(tag) for wd, tag in words_tags]\n",
    "    return \" \".join(lemma_list)\n",
    "\n",
    "print(pos_tagger(drop_odd))\n",
    "# # Lemmatize\n",
    "# # drop_odd = resume_file\n",
    "# lemma_list = pos_tagger(drop_odd)\n",
    "# lemmatized_sentence = \" \".join(lemma_list)\n",
    "# print(lemmatized_sentence)\n",
    "#> the bat saw the cat with stripe hang upside down by their foot\n",
    "\n",
    "# lemmatized_sentence = \" \".join([w.lemmatizer for w in TextBlob.words])\n",
    "# print(lemmatized_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pattern3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATTERN LEMMATIZER\n",
    "import pattern3\n",
    "from pattern3.en import lemma, lexeme\n",
    "from pattern3.en import parse\n",
    "from pprint import pprint\n",
    "\n",
    "sentence = resume_file\n",
    "\n",
    "lemmatized_sentence = \" \".join([lemma(word) for word in sentence.split()])\n",
    "\n",
    "# print(lemmatized_sentence)\n",
    "#> the bat see the cat with best stripe hang upside down by their feet\n",
    "\n",
    "# Special Feature : to get all possible lemmas for each word in the sentence\n",
    "all_lemmas_for_each_word = [lexeme(wd) for wd in lemmatized_sentence.split()]\n",
    "# pprint(all_lemmas_for_each_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking to see our most common words\n",
    "filtered_text = nltk.FreqDist(filtered_text)\n",
    "print(filtered_text.most_common(15))\n",
    "\n",
    "word_features = list(filtered_text.keys())[:3000]\n",
    "\n",
    "\n",
    "# def find_features(document):\n",
    "#     words = set(document)\n",
    "#     features = {}\n",
    "#     for w in word_features:\n",
    "#         # creates a boolean for if the word is in the list of words, it will flag true\n",
    "#         features[w] = (w in words)\n",
    "#     return features\n",
    "\n",
    "\n",
    "# find_features()\n",
    "print(word_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# use regex to split text into words and punctuation\n",
    "# global drop_odd\n",
    "\n",
    "\n",
    "# for loop that runs over every word in test\n",
    "resume_dict = {}\n",
    "drop_odd_sep = re.findall(r\"[\\w']+|[.,!?;]\", str(drop_odd))\n",
    "def marc_dict():\n",
    "    pastWord = \"\"\n",
    "    #nextWord is the word you're on\n",
    "    for nextWord in drop_odd_sep:\n",
    "        if pastWord in resume_dict:\n",
    "            frequency = resume_dict[pastWord]\n",
    "        else:\n",
    "            frequency = {}\n",
    "            resume_dict[pastWord] = frequency\n",
    "        if nextWord in frequency:\n",
    "            occurrances = frequency[nextWord]\n",
    "            #value; either the number that's already there or the one we just put there\n",
    "        else:\n",
    "            occurrances = 0\n",
    "        frequency[nextWord] = occurrances + 1\n",
    "    pastWord = nextWord\n",
    "    return resume_dict\n",
    "# print(drop_odd)\n",
    "marc_dict()\n",
    "# name our new dictionary of words and count of next possible words to a new variable\n",
    "resume_dict = marc_dict()\n",
    "print(json.dumps(resume_dict, sort_keys = True, indent = 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"start the story\"\"\"\n",
    "\n",
    "# name variable with the first word in our list of words\n",
    "first_word = random.choice(seuss_string)\n",
    "print(first_word)\n",
    "# name the first word of our story as\n",
    "global story\n",
    "story = []\n",
    "\n",
    "\n",
    "def start_story(list):\n",
    "    \"\"\"function to start the story\"\"\"\n",
    "    if story == []:\n",
    "        # set story up with a first word\n",
    "        story.append(first_word)\n",
    "    else:\n",
    "        # dont start the story if its already been started\n",
    "        pass\n",
    "    return story\n",
    "\n",
    "\n",
    "start_story(seuss_string)\n",
    "# call function on our list of words from the text\n",
    "seuss_list = seuss_string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def whos_next():\n",
    "    \"\"\"function to get the next word using weighted probability\"\"\"\n",
    "    current_word = story[-1]\n",
    "    for outer_word, inner_dict in seuss_dict.items():\n",
    "        # pick out our nested dict for specific word\n",
    "        word_self_total = inner_dict.items()\n",
    "        # get the next words and instances\n",
    "        total_occurances = inner_dict.values()\n",
    "        # seperate the instances\n",
    "        if outer_word == current_word:\n",
    "            # pull the dictionary for the current word\n",
    "            total = sum(total_occurances)\n",
    "            # add the total times a word followed current word\n",
    "            probs = []\n",
    "            # set empty variable for the percent chance of each word occuring\n",
    "            hopefuls = []\n",
    "            # set a list of all words that came after current word\n",
    "            likely = []\n",
    "            # set an empty list to grab the count of each word\n",
    "            for number in total_occurances:\n",
    "                # pull every number of times each word came after current\n",
    "                percent = number / total\n",
    "                # factor the percent chance of a word being next\n",
    "                probs.append(percent)\n",
    "                # add each percent to our probs list\n",
    "\n",
    "            for word, num in word_self_total:\n",
    "                # pull words and their values from our inner dict\n",
    "                hopefuls.append(word)\n",
    "                # add each word to our hopefuls list\n",
    "                likely.append(num)\n",
    "                # add each number to our likely list\n",
    "                global rng_says\n",
    "\n",
    "            rng_says = choice(hopefuls, p=probs)\n",
    "            # weighted rng based off the percent probability of word occuring\n",
    "            # return rng_says\n",
    "\n",
    "            next_word = str(rng_says)\n",
    "            # change rng output from numpy to string\n",
    "            story.append(next_word)\n",
    "            # add our next word string to the story\n",
    "            # current_word = story[-1]\n",
    "            # set current word to be the last word of the story\n",
    "\n",
    "\n",
    "whos_next()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"add words to the story\"\"\"\n",
    "\n",
    "\n",
    "while len(story) < 100:\n",
    "    whos_next()\n",
    "whole_story = ' '.join(story)\n",
    "print(whole_story)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "whole_story = re.sub(r'\\s([?.,!](?:\\s|$))', r'\\1', whole_story)\n",
    "\n",
    "\n",
    "print(whole_story)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaned_story = '.'.join(map((lambda x: x[0].upper()+x[1:])))\n",
    "\n",
    "# cap_sentences = [sentences.capitalize() for sentences in whole_story]\n",
    "sentences = re.split('[?.]', whole_story)\n",
    "\n",
    "\n",
    "def cap():\n",
    "    for string in whole_story:\n",
    "        return string[:1].capitalize() + string[1:].lower()\n",
    "\n",
    "\n",
    "print(sentences)\n",
    "# cap_sentences = [cap(sentences) for sentences in whole_story]\n",
    "\n",
    "# print(cap_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in story:\n",
    "    if i in puncuation:\n",
    "        x = story[i+1]\n",
    "        x.capatilize()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4ef022e24af0d9896d2df829234e3a8e308e976d2609f0b1a6e645a4d7a45074"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
