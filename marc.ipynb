{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "import re\n",
    "import string\n",
    "from numpy.random import choice\n",
    "import random\n",
    "import PyPDF2\n",
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "# nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# nltk.download('wordnet')\n",
    "\"\"\"wordnet_tagged = list(map(lambda x: (x[0], pos_tagger(x[1])), pos_tagged))\n",
    "\"\"\"\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "\"\"\"create a dict of how what words follow which and how many times they did\"\"\"\n",
    "with open('data/Resume/test/test_resumes.txt', 'r') as resume_script:\n",
    "    # open txt file and read to string, string to lower\n",
    "    resume_file = resume_script.read()\n",
    "    resume_file = resume_file.lower()\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_words = word_tokenize(resume_file)\n",
    "lemmings = ' '.join([lemmatizer.lemmatize(w) for w in resume_words])\n",
    "# for words in resume_words:\n",
    "#     print(words + ' ---> ' + lemmatizer.lemmatize(words))\n",
    "print(lemmings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"stopwords\"\"\"\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "# stop word filtration\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "# filtered_sentence = []\n",
    "# for w in words:\n",
    "#     if w not in stop_words:\n",
    "#         filtered_sentence.append(w)\n",
    "\"\"\"or do the lambda\"\"\"\n",
    "\n",
    "filtered_sentence = [w for w in words if not w in stop_words]\n",
    "print(filtered_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# cleared our test data of stop words\n",
    "# words = word_tokenize(resume_file)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "token_lemmings = word_tokenize(lemmings)\n",
    "\n",
    "stopped_lemmings = [w for w in token_lemmings if not w in stop_words]\n",
    "print(type(stopped_lemmings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = set(nltk.corpus.words.words())\n",
    "\n",
    "drop_odd = [\" \".join(w for w in nltk.wordpunct_tokenize(str(stopped_lemmings)) if w in words)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to lemmatize each word with its POS tag\n",
    "\n",
    "# POS_TAGGER_FUNCTION : TYPE 1\n",
    "def pos_tagger(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pos_tagged = nltk.pos_tag(nltk.word_tokenize(resume_file))\n",
    "print(pos_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_148782/913534261.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m# else use the tag to lemmatize the token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mwn_tagged_sent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mwn_tagged_sent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwn_tagged_sent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deb/tests/capstone/venv/lib/python3.7/site-packages/nltk/stem/wordnet.py\u001b[0m in \u001b[0;36mlemmatize\u001b[0;34m(self, word, pos)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mlemma\u001b[0m \u001b[0mof\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mword\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgiven\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \"\"\"\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mlemmas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_morphy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemmas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlemmas\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deb/tests/capstone/venv/lib/python3.7/site-packages/nltk/corpus/reader/wordnet.py\u001b[0m in \u001b[0;36m_morphy\u001b[0;34m(self, form, pos, check_exceptions)\u001b[0m\n\u001b[1;32m   2006\u001b[0m         \u001b[0;31m#    find a match or you can't go any further\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2007\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2008\u001b[0;31m         \u001b[0mexceptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2009\u001b[0m         \u001b[0msubstitutions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMORPHOLOGICAL_SUBSTITUTIONS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "# we use our own pos_tagger function to make things simpler to understand.\n",
    "wordnet_tagged = list(map(lambda x: (x[0], pos_tagger(x[1])), pos_tagged))\n",
    "\n",
    "\n",
    "wn_tagged_sent = []\n",
    "for word, tag in wordnet_tagged:\n",
    "    if tag is None:\n",
    "        # if there is no available tag, append the token as is\n",
    "        wn_tagged_sent.append(word)\n",
    "    else:\n",
    "        # else use the tag to lemmatize the token\n",
    "        wn_tagged_sent.append(lemmatizer.lemmatize(word, tag))\n",
    "wn_tagged_sent = \" \".join(wn_tagged_sent)\n",
    "\n",
    "print(wn_tagged_sent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "# Define function to lemmatize each word with its POS tag\n",
    "\n",
    "# POS_TAGGER_FUNCTION : TYPE 2\n",
    "\n",
    "\n",
    "def pos_tagger(sentence):\n",
    "    sent = TextBlob(sentence)\n",
    "    tag_dict = {\"J\": 'a', \"N\": 'n', \"V\": 'v', \"R\": 'r'}\n",
    "    words_tags = [(w, tag_dict.get(pos[0], 'n')) for w, pos in sent.tags]\n",
    "    lemma_list = [wd.lemmatize(tag) for wd, tag in words_tags]\n",
    "    return lemma_list\n",
    "\n",
    "\n",
    "# Lemmatize\n",
    "sentence = resume_file\n",
    "lemma_list = pos_tagger(sentence)\n",
    "lemmatized_sentence = \" \".join(lemma_list)\n",
    "print(lemmatized_sentence)\n",
    "#> the bat saw the cat with stripe hang upside down by their foot\n",
    "\n",
    "lemmatized_sentence = \" \".join([w.lemmatize() for w in t_blob.words])\n",
    "print(lemmatized_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pattern3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATTERN LEMMATIZER\n",
    "import pattern3\n",
    "from pattern3.en import lemma, lexeme\n",
    "from pattern3.en import parse\n",
    "from pprint import pprint\n",
    "\n",
    "sentence = resume_file\n",
    "\n",
    "lemmatized_sentence = \" \".join([lemma(word) for word in sentence.split()])\n",
    "\n",
    "# print(lemmatized_sentence)\n",
    "#> the bat see the cat with best stripe hang upside down by their feet\n",
    "\n",
    "# Special Feature : to get all possible lemmas for each word in the sentence\n",
    "all_lemmas_for_each_word = [lexeme(wd) for wd in lemmatized_sentence.split()]\n",
    "# pprint(all_lemmas_for_each_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(',', 457), ('.', 236), ('art', 65), ('state', 59), ('education', 52), ('city', 51), ('arts', 49), ('company', 47), ('school', 42), ('name', 35), ('student', 32), ('food', 31), ('customer', 31), ('-', 31), ('ï¼\\u200b', 30)]\n",
      "['tutoring', 'consultantexecutive', 'profile', 'motivated', ',', 'enthusiastic', 'educational', 'leader', '15+', 'years', \"'\", 'experience', 'fostering', 'cohesive', 'student', 'learning', 'atmosphere', 'conducive', '.', 'â', 'core', 'qualifications', 'include', 'licensed', 'reading', 'specialist', 'multiple', 'states', 'strong', 'classroom', 'management', 'public', 'speaking', 'skills', 'cpr', 'certified', 'f.e.m.a', 'emergency', 'situations', 'skill', 'highlights', 'leadership/communication', 'self-motivated', 'crisis', 'patient', 'accomplishments', 'helped', 'low', 'functioning', 'readers', 'move', 'pre-k', 'level', '5th', '6th', 'grade', 'levels', 'facilitated', 'small', 'writing', 'groups', 'phonics', 'grades', '6', '12', 'managed', 'classes', '45.', 'chaperoned', 'field', 'trips', 'after-school', 'activities', 'designed', 'weekly', 'lesson', 'plans', 'focused', 'age', 'level-appropriate', 'material', 'curriculum', 'based', 'upon', 'needs', 'myâ', 'students', 'reach', 'iep', 'goals', 'professional', 'consultant', 'august', '2014', 'current', 'company', 'name', 'ï¼\\u200b', 'city', 'state', 'working', 'special', 'promoted', 'language', 'development', 'storytelling', 'applied', 'positive', 'reinforcement', 'method', 'redirect', 'negative', 'behaviors', 'encouraged', 'understanding', 'helpful', 'others', 'observed', 'supply', 'teachers', 'feedback', 'regarding', 'potential', 'blocks', 'opportunities', 'support', 'english/social', 'studies', 'september', '2013', 'developed', 'implemented', 'daily', 'unit', 'english', 'administered', 'corrected', 'tests', 'quizzes', 'timely', 'manner', 'conducted', 'group', 'one-on-one', 'excellent', 'rapport', 'fostered', 'environment', 'promoting', 'engagement', 'participated', 'parent', 'faculty', 'conferences', 'instructor', 'voyager', 'program', 'pbis', 'programs', 'encourage', 'participation', 'supported', 'developing', 'strategies', 'individual', 'dynamics', 'communicated', 'effectively', 'educators', 'various', 'physically', 'verbally', 'interacted', 'throughout', 'day', 'keep', 'engaged', 'supplied', 'attention', 'maintaining', 'overall', 'focus', 'entire', 'differentiated', 'parents', 'school', 'year', 'created', 'team', 'meeting', 'format', 'allow', 'opportunity', 'meet', 'child', \"'s\", 'together', 'social', 'teacher', 'june', '2007', 'provided', 'mandated', '10th', 'world', 'history', 'well', 'advanced', 'placement', 'global', 'regents', 'study', 'habits', 'saturdays', 'inter-disciplinary', 'portfolio', 'projects', 'selected', 'theme', 'reinforced', 'needed', 'successful', 'testing', 'tracked', 'analyzed', 'class', 'performance', 'workshop', 'model', 'familiar', 'danielson', 'framework', 'used', 'design', 'create', 'avid', 'cornell', 'notes', 'binder', 'organization', 'turn-keyed', 'training', 'staff', 'established', 'enforced', 'rules', 'behavior', 'procedures', 'order', 'among', '34', 'arts', 'teacher/social', '2002', 'maintained', 'progress', 'portfolios', '7th', 'america', 'choice', 'writers', 'coordinated', 'hours', 'help', 'need', 'extra', 'prevention', '2000', 'federally', 'presentations', 'performed', 'background', 'reviews', 'develop', 'tailored', 'lessons', 'worked', 'discussion', 'counseling', 'sessions', 'organized', 'school-wide', 'including', 'bake', 'sales', 'closely', 'administration', 'interesting', 'interactive', 'mediums', 'increase', 'course', 'materials', 'alumni', 'peer', 'helper', 'january', '1997', 'department', 'assistant', 'principals', 'assisted', 'intervention', 'specialists', 'community', 'education', 'master', 'science', ':', 'district', '2010', 'touro', 'college', 'united', 'literacy', '2005', 'coursework', 'bachelor', 'political', 'st.', 'francis', 'psychology', 'member', 'phi', 'alpha', 'theta', 'pi', 'sigma', 'thomas', 'moore', 'pre-law', 'society', 'affiliations', 'national', 'association', 'american', 'psychological', 'irish', 'emerald', 'nyc', 'cultural', 'committee', 'economic', 'justice', 'leadership', 'positions', 'f.i.t', '(', 'data', 'inquiry', ')', 'brinkley', 'middle', '2014-2016', 'coordinator', 'ask', '2013-2016', 'interventionist', '2013-2015', 'volunteer', 'salvation', 'army', '1993-2013', 'served', 'bryant', 'high', '1997-2002', 'comprehensive', 'plan', 'intermediate', '141', '2002-2007', 'big', 'brothers/', 'sisters', '2003-2007', 'analysis', '2006-2007', 'academic', 'services', 'academy', 'finance', 'enterprise', '2007-2013', '2007-', 'coach', 'site', '2009-2013', 'chair', '2007-2009', '2010-2012', 'building', 'response', '2011-2013', 'safety', 'queens', 'campus', 'schools', '2011-2012', 'cabinet', 'senior', 'adviser', '2009-2012', 'cosa', 'saturday', 'preparation', 'people', 'person', 'goal', 'oriented', 'player', 'additional', 'information', 'n.y.s', 'permanent', 'license', '7-12', 'k-12', 'mississippi', 'k-12â', 'food', 'serversummary', 'sixteen', 'customer', 'service', 'industry', 'includes', 'customer-oriented', 'server', 'crew', 'trainer', 'housekeeper', 'deep', 'pos', 'systems', 'cash', 'handling', 'four', 'microsoft', 'office', 'adobe', 'media', 'software', 'excel', 'access', 'word', 'powerpoint', 'photoshop', 'illustrator', 'math', 'multi-tasker', 'conversant', 'spanishdelivers', 'exceptional', 'comfortable', 'standing', 'long', 'time', 'periods', 'neat', 'clean', 'appearance', 'reliable', 'punctual', '-', 'lucky', 'lill', 'casino', 'took', 'tips', 'passed', 'perfect', 'score', 'awarded', 'certificate', 'hourly', 'raise', '2003', 'denny', 'restaurant', 'r', 'ecognized', 'going', 'beyond', 'normal', 'job', 'functions', 'owners', 'title', '``', 'duties', 'included', 'new', 'wait-staff', 'employees', 'proper', 'recorded', 'orders', 'repeated', 'back', 'clear', 'understandable', 'manner.up-sold', 'menu', 'items', 'beverages', 'desserts', 'profits.correctly', 'received', 'processed', 'payments', 'responded', 'appropriately', 'guest', 'concerns.served', 'fresh', 'hot', 'smile', 'manner.resolved', 'complaints', 'promptly', 'professionally.prepared', 'coffee', 'tea', 'fountain', 'drinks.mastered', 'point', 'sale', 'computer', 'system', 'automated', 'taking.frequently', 'washed', 'sanitized', 'hands', 'areas', 'tools.maintained', 'groomed', 'impeccable', 'personal', 'hygiene', 'hair', 'restraint', 'minimal', 'jewelry', 'met', 'standards.worked', 'teammates', 'openly', 'invited', 'coaching', 'team.prepared', 'according', 'written', 'verbal', 'several', 'different', 'simultaneously', 'cashier', '1998', 'mastered', 'taking', 'concerns', 'up-sold', 'profits', 'properly', 'portioned', 'packaged', 'take-out', 'foods', 'customers', 'necessary', 'steps', 'resolve', 'issues', 'clearly', 'positively', 'co-workers', 'housekeeping', '2004', 'hand', 'dusted', 'wiped', 'furniture', 'fixtures', 'window', 'sills.removed', 'finger', 'marks', 'smudges', 'vertical', 'surfaces', 'doors', 'frames', 'glass', 'partitions.swept', 'damp-mopped', 'private', 'stairways', 'hallways.cleaned', 'exterior', 'lighting', 'plastic', 'enclosures.emptied', 'cleaned', 'waste', 'receptacles.cleaned', 'returned', 'vacant', 'rooms', 'occupant-ready', 'status.stocked', 'toilet', 'tissue', 'paper', 'towels', 'restroom', 'supplies.supplied', 'guests', 'toiletries', 'requested.stocked', 'room', 'attendant', 'carts', 'supplies.removed', 'trash', 'dirty', 'linens', 'carts.swept', 'vacuumed', 'floors', 'hallways', 'stairwells.cleaned', 'satisfaction', 'clients', 'correctly', 'reported', 'windows', 'counters', 'tables.mastered', 'tutor', 'tutored', 'aftereffects', 'assignments', '&', 'graveyard', 'concerns.recorded', 'manner.took', 'issues.up-sold', 'profits.served', 'manner.communicated', 'management.resolved', 'drinks.carefully', 'sanitation', 'health', 'standards', 'work', 'areas.mastered', 'taking.maintained', 'standards.followed', 'practices', 'procedures.worked', 'supervisor', '2006', 'prepared', 'drinks', 'resolved', 'professionally', 'carefully', 'frequently', 'tools', 'shift', 'books', 'accurately', 'nightly', 'basis', 'followed', 'runner', 'educated', 'game', 'mathematical', 'probabilities', 'wagers', 'oversaw', 'cage', 'operations', 'paid', 'bets', 'retrieved', 'cards', 'beverage', 'increased', 'volume', 'loyalty', 'attracting', 'players', 'relationships', 'advances', 'credit', 'standard', 'safe', 'assembly', 'presentation', 'ensure', 'associate', '/', 'university', 'montana', 'missoula', 'currently', 'attending', 'pursuit', 'degree', 'e-mail', 'phone', 'type', 'tooling', 'control', 'leadsummary', 'qualifiedâ', 'manufacturing', 'manager', 'electrician', 'stays', 'security', 'safely', 'operates', 'maneuvers', 'diverse', 'range', 'heavy', 'duty', 'construction', 'equipment', 'looking', 'long-term', 'position', 'values', 'organizational', 'culture', 'integrity', 'forklift', 'operator', 'hoe', 'electric', 'palette', 'jack', 'pipe', 'laying', 'threading', 'bending', 'trained', 'blueprint', 'driving', 'record', 'communication', 'lead', '01/2012', 'achieve', 'supporting', 'production', 'workers.â', 'identified', 'change', 'workplace', 'policy', 'procedure', 'effect', 'rate', 'quality', 'supervised', 'ofâ', 'eight', 'skilled', 'machine', 'operators', 'inspected', 'finished', 'products', 'adherence', 'specifications', 'monitored', 'processes', 'adjusted', 'schedules', 'adhered', 'applicable', 'regulations', 'policies', 'environmental', 'compliance', 'managers', 'implement', 'troubleshooted', 'problems', 'devices', 'operated', 'machinery', 'aâ', 'hem', 'saw', 'sheer', 'table', 'chop', 'drill', 'recipicating', 'bale', 'inspector', '01/2011', 'changed', 'product', 'feed', 'speed', 'malfunctioned', 'workers', 'started', 'inserting', 'instructions', 'units', 'chef', 'consistently', 'kept', 'adhering', 'federal', 'local', 'requirements', 'ensured', 'smooth', 'kitchen', 'operation', 'overseeing', 'inventory', 'purchasing', 'receiving.â', 'inspections', 'reports', 'displayed', 'friendly', 'attitude', 'towards', 'fellow', 'members', 'apprentice', '01/2008', '01/2009', 'value', 'client', 'base', 'vast', 'knowledge', 'electronics', 'principles', 'complex', 'electrical', 'accurate', 'electronic', 'instrument', 'efficient', 'overtime', 'weekends', 'holidays', 'co-owner', 'martial', '01/2007', 'taught', 'modify', 'exercises', 'avoid', 'injury', 'contributed', 'club', 'engage', 'fitness', 'gym', 'led', 'enhancement', 'improvingâ', 'artsâ', 'success', 'forman', '01/2005', 'efforts', 'guarantee', 'completed', 'budget-conscious', 'open', 'effective', 'communications', 'project', 'teams', 'productivity', 'interpreted', 'mechanical', 'schematics', 'blueprints', 'diagrams', 'diagnosed', 'malfunctions', 'repairs', 'restore', 'maintain', 'uptime', 'installed', 'wiring', 'remodeling', '01/1999', '01/2004', 'laid', 'threaded', 'abilityâ', 'operate', 'forklifts', 'backhoes', 'jacks', 'proficient', 'estitrack', 'ax', 'scheduling', 'managing', 'floor', 'efficiently', 'able', 'provide', '20', 'installation', 'underground', 'utilities', 'piping', 'ability', 'install', 'meter', 'centers', 'panels', 'favorably', 'introduce', 'fitter', 'â€‹', 'diploma', 'northside', 'center', 'electricians', 'northeast', 'flordia', 'builders', 'usa', 'teachersummary', 'visual', '11', 'elementary', 'art', 'demonstrated', 'individualizing', 'instruction', \"students'\", 'interests', 'creative', 'cloud', 'indesign', 'muse', 'wordpress', 'imovie', 'ohio', 'teaching', 'licensure', 'pk-12', 'october', 'oak', 'hills', 'evaluated', 'artwork', 'stated', 'criteria', 'initiated', 'arranged', 'trip', 'cincinnati', 'museum', 'contemporary', 'extension', 'assessed', 'planned', 'sculpture', 'exhibit', 'delhi', 'library', '1-8', 'teach', 'interdisciplinary', 'integrate', 'technology', 'evaluate', 'prepare', 'quarterly', 'report', 'display', 'biannual', 'all-school', 'exhibits', 'held', 'catholic', 'week', 'house', 'annual', 'fine', 'festival', 'participate', 'regional', 'competitions', 'exhibitions', 'scholastic', 'competition', 'youth', 'month', 'exhibition', 'young', 'peoples', 'catholics', 'poor', 'drawing', 'knights', 'columbus', 'substance', 'abuse', 'awareness', 'poster', 'facilitate', 'criticism', 'submissions', 'jerry', 'tollifson', 'oaea', 'workshops', 'manage', 'website', 'online', 'gallery', 'blog', 'www.olvisitation.com', 'xavier', 'may', '2012', 'masters', 'secondary', 'mount', 'joseph', 'painting', '1989', 'graphic', 'swoaea', 'summer', 'institute', 'evenings', 'awards', 'parallel', 'visions', \"''\", 'studio', 'san', 'guiseppe', 'pendleton', 'exhibitor', 'july', 'december', 'life', 'times', 'juried', 'strs', '2001', 'award', 'winner', 'msj', 'thesis', 'com', 'conventions', 'naea', 'present', 'southwest', 'taft', 'nuts', 'bolts', 'exchange', 'presenter', 'michael', 'crafts', 'storesummary', 'determined', 'hard-working', 'zeal', 'accomplish', 'task', 'challenge', \"'m\", 'given', 'seeking', 'exceed', 'expectations', 'next', 'attain', 'sustainable', 'technologies', 'store', 'march', '2017', 'capital', 'blvd', 'location', 'aid', 'full-time', 'replenishment', 'planning', ';', 'unload', 'stock', 'merchandise', 'train', 'associates', 'employee', 'february', '2016', 'day/night-time', 'assess', 'assumed', 'brier', 'creek', 'raleigh', 'nc', '2015', 'registers', 'count', 'money', 'documentation', 'bank', 'deposit', 'cashiers', 'morning', 'associate/cashier', 'offer', 'run', 'register', 'certifications', 'arcgis', 'water', 'stream', 'assessment', 'tree', 'osha', '40', 'hour', 'hazwoper', 'certification', 'liberal', 'mathematics', 'suny', 'adirondack', 'dean', 'list', 'graduated', 'cum', 'laude', '2018', 'north', 'carolina', 'richard', 'r.', 'lee', 'estate', 'clayton', 'phase', 'iii', 'line', 'attendantsummary', 'results-oriented', 'dedicated', 'providing', 'making', 'operational', 'procedural', 'improvements', 'certifications/skills', 'accuracy', 'talent', 'obtaining/charting', 'vital', 'signs', 'cpr/first', 'opening', 'closing', 'outstanding', 'competitive', 'make', 'recommendations', 'future', 'growth', 'nov', '2009', 'sep', 'directed', 'inventory-taking', 'reconciling', 'receipts', 'performing', 'offered', 'differentiate', 'promote', 'brand', 'assigned', 'specific', 'scheduled', 'break', 'assuring', 'go', 'accordingly', 'receive', 'satisfactory', 'goods', 'recommend', 'locate', 'obtain', 'desires', 'called', 'stores', 'within', 'area', 'find', 'desired', 'try', 'fit', 'greeted', 'ascertain', 'wants', 'telephone', 'requests', 'particular', 'promotions', 'payment', 'exchanges', 'answered', 'calls', 'unloaded', 'picked', 'staged', 'loaded', 'shipping', 'rotated', 'code', 'receiving', 'date', 'transported', 'racks', 'shelves', 'vehicles', 'replenished', 'shipments', 'availability', 'upheld', 'pricing', 'lines', 'flow', 'storage', 'fulfillment', 'rapid', 'pace', 'tight', 'deadlines', 'banded', 'wrapped', 'cleared', 'apr', 'feb', 'cooked', 'corporate', 'guidelines', 'temperature', 'gauge', 'fda', 'places', 'bag', 'delivers', 'assurance', 'reference', 'serving', 'refreshments', 'verified', 'drive-in', 'read', 'slip', 'required', 'patron', 'medical', 'externship', 'aug', '2008', 'general', 'answering', 'telephones', 'dictation', 'completing', 'insurance', 'forms', 'appointments', 'showed', 'patients', 'examination', 'physician', 'sterilized', 'instruments', 'disposed', 'contaminated', 'supplies', 'interviewed', 'measured', 'weight', 'height', 'statistics', 'test', 'results', 'records', 'collected', 'blood', 'laboratory', 'specimens', 'logged', 'seasonal', 'culinary', 'cafã©', 'hostess', 'oct', 'sold', 'using', 'delivered', 'kitchens', 'ready', 'brewed', 'filled', 'containers', 'requested', 'scrubbed', 'polished', 'steam', 'tables', 'glasses', 'dishes', 'jan', 'described', 'explained', 'use', 'care', 'recommended', 'ascertained', 'wanted', 'ticketed', 'computed', 'prices', 'totaled', 'purchases', 'sanford-brown', 'us', 'trevose', 'pa', 'receptionist', 'retail', 'cpr/', 'instructorsummary', '22', 'addressing', 'ensuring', 'formative', 'assessments', 'student-centered', 'innovations', 'implementation', 'across', 'content', 'fields', 'adept', 'differentiating', 'data-driven', 'expertise', 'disabilities', 'techniques', 'documented', 'kind', 'empathetic', 'urban', '08/1990', '05/2012', 'instructional', 'consistent', 'tempe', 'arizona', 'board', 'specializing', 'photography', 'serigraphy', 'ceramics', 'mural', 'mask', 'mosaics', 'commercial', 'applications', 'recognized', '150', 'audio', 'aids', 'stimulation', 'imagination', 'good', 'conflicting', 'priorities', 'demanded', 'flexible', 'problem', 'solving', 'abilities', 'regular', 'courses', 'up-to-date', 'methods', 'developments', 'variety', 'lectures', 'discussions', 'demonstrations.presented', '8', 'semester', '01/1998', '10', 'individually', 'claymation', 'silk', 'screening', 'employed', 'improve', 'sensory/perceptual-motor', 'cognition', 'memory', 'balanced', 'demonstration', 'observe', 'question', 'investigate', 'setting', 'objectives', 'lessons/projects', 'achieving', 'total', 'grading', 'rubric', 'integration', 'scores', 'introducing', 'relevant', 'interest', 'enjoyment', '1973', 'honors', 'specialized', 'printmaking', 'structured', 'immersion', '1995', 'conference', 'chicago', 'york', 'phoenix', 'francisco', 'president', '2000-', 'division', '1992-1994', 'stanford', 'accelerated', 'participant', 'alliance', 'achievement', 'rural', 'without', 'international', 'non-government', 'artist-in', 'residency', 'piaute', 'paradise', 'valley', 'lifetime', 'presented', 'diablos', 'grady', 'gammage', 'artist', 'implementing', 'inside', 'guadalupe', 'hbo', 'documentary', 'j.r.', 'discovery', 'meaning', 'published', 'reston', 'virginia', '2005-2006', 'pta', 'recognition', 'fees', '2004-2005', 'excellence', 'wrote', 'winning', 'cross-content', 'curricular', 'adventures', 'combining', 'computers', 'home', 'economics', 'creation', 'restaurants', '300', 'family', 'friends', 'magazine', 'rites', 'passage', 'children', 'research', 'humor', '1996', 'publication', 'observation', 'integrates', 'multiculturalism', 'pacific', 'region', 'states/territories', 'educator', 'cultures', 'nba/coca', 'cola', 'stay', 'star', '2000-2002', 'built', 'animation', 'serrigraphy', 'utilizing']\n"
     ]
    }
   ],
   "source": [
    "# checking to see our most common words\n",
    "filtered_text = nltk.FreqDist(filtered_text)\n",
    "print(filtered_text.most_common(15))\n",
    "\n",
    "word_features = list(filtered_text.keys())[:3000]\n",
    "\n",
    "\n",
    "# def find_features(document):\n",
    "#     words = set(document)\n",
    "#     features = {}\n",
    "#     for w in word_features:\n",
    "#         # creates a boolean for if the word is in the list of words, it will flag true\n",
    "#         features[w] = (w in words)\n",
    "#     return features\n",
    "\n",
    "\n",
    "# find_features()\n",
    "print(word_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    # use regex to split text into words and punctuation\n",
    "# global resume_string\n",
    "resume_string = re.findall(r\"[\\w']+|[.,!?;]\", resume_file)\n",
    "\n",
    "# for loop that runs over every word in test\n",
    "resume_dict = {}\n",
    "\n",
    "for i, word in enumerate(resume_string[:-1]):\n",
    "    this_word = resume_string[i+1]\n",
    "    # if this_word is in dict, asssign it to 'next_count'\n",
    "    if this_word not in resume_dict:\n",
    "        next_count = {}\n",
    "        resume_dict[this_word] = next_count\n",
    "\n",
    "    # if not, create empty dictionary with this_word as the key and next_count as the value\n",
    "    else:\n",
    "        next_count = resume_dict[this_word]\n",
    "\n",
    "    if word in next_count:\n",
    "        next_count[word] += 1\n",
    "    else:\n",
    "        next_count[word] = 1\n",
    "this_word = resume_string[i+1]\n",
    "\n",
    "# print(resume_string)\n",
    "\n",
    "# name our new dictionary of words and count of next possible words to a new variable\n",
    "\n",
    "# print(resume_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"start the story\"\"\"\n",
    "\n",
    "# name variable with the first word in our list of words\n",
    "first_word = random.choice(seuss_string)\n",
    "print(first_word)\n",
    "# name the first word of our story as\n",
    "global story\n",
    "story = []\n",
    "\n",
    "\n",
    "def start_story(list):\n",
    "    \"\"\"function to start the story\"\"\"\n",
    "    if story == []:\n",
    "        # set story up with a first word\n",
    "        story.append(first_word)\n",
    "    else:\n",
    "        # dont start the story if its already been started\n",
    "        pass\n",
    "    return story\n",
    "\n",
    "\n",
    "start_story(seuss_string)\n",
    "# call function on our list of words from the text\n",
    "seuss_list = seuss_string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def whos_next():\n",
    "    \"\"\"function to get the next word using weighted probability\"\"\"\n",
    "    current_word = story[-1]\n",
    "    for outer_word, inner_dict in seuss_dict.items():\n",
    "        # pick out our nested dict for specific word\n",
    "        word_self_total = inner_dict.items()\n",
    "        # get the next words and instances\n",
    "        total_occurances = inner_dict.values()\n",
    "        # seperate the instances\n",
    "        if outer_word == current_word:\n",
    "            # pull the dictionary for the current word\n",
    "            total = sum(total_occurances)\n",
    "            # add the total times a word followed current word\n",
    "            probs = []\n",
    "            # set empty variable for the percent chance of each word occuring\n",
    "            hopefuls = []\n",
    "            # set a list of all words that came after current word\n",
    "            likely = []\n",
    "            # set an empty list to grab the count of each word\n",
    "            for number in total_occurances:\n",
    "                # pull every number of times each word came after current\n",
    "                percent = number / total\n",
    "                # factor the percent chance of a word being next\n",
    "                probs.append(percent)\n",
    "                # add each percent to our probs list\n",
    "\n",
    "            for word, num in word_self_total:\n",
    "                # pull words and their values from our inner dict\n",
    "                hopefuls.append(word)\n",
    "                # add each word to our hopefuls list\n",
    "                likely.append(num)\n",
    "                # add each number to our likely list\n",
    "                global rng_says\n",
    "\n",
    "            rng_says = choice(hopefuls, p=probs)\n",
    "            # weighted rng based off the percent probability of word occuring\n",
    "            # return rng_says\n",
    "\n",
    "            next_word = str(rng_says)\n",
    "            # change rng output from numpy to string\n",
    "            story.append(next_word)\n",
    "            # add our next word string to the story\n",
    "            # current_word = story[-1]\n",
    "            # set current word to be the last word of the story\n",
    "\n",
    "\n",
    "whos_next()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"add words to the story\"\"\"\n",
    "\n",
    "\n",
    "while len(story) < 100:\n",
    "    whos_next()\n",
    "whole_story = ' '.join(story)\n",
    "print(whole_story)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "whole_story = re.sub(r'\\s([?.,!](?:\\s|$))', r'\\1', whole_story)\n",
    "\n",
    "\n",
    "print(whole_story)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaned_story = '.'.join(map((lambda x: x[0].upper()+x[1:])))\n",
    "\n",
    "# cap_sentences = [sentences.capitalize() for sentences in whole_story]\n",
    "sentences = re.split('[?.]', whole_story)\n",
    "\n",
    "\n",
    "def cap():\n",
    "    for string in whole_story:\n",
    "        return string[:1].capitalize() + string[1:].lower()\n",
    "\n",
    "\n",
    "print(sentences)\n",
    "# cap_sentences = [cap(sentences) for sentences in whole_story]\n",
    "\n",
    "# print(cap_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in story:\n",
    "    if i in puncuation:\n",
    "        x = story[i+1]\n",
    "        x.capatilize()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4ef022e24af0d9896d2df829234e3a8e308e976d2609f0b1a6e645a4d7a45074"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
